{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“ML.H3.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjlala/Android_lab/blob/master/%E2%80%9CML_H3_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otg4b0Mraa5l"
      },
      "source": [
        "# **Homework Assignment #3**\n",
        "\n",
        "Assigned: February 19, 2021\n",
        "\n",
        "Due: March 5, 2021\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(12 points) Using the graphs below, draw the decision boundaries for the following binary classifiers:\n",
        "\n",
        "- decision tree\n",
        "- perceptron\n",
        "- k nearest neighbors (k=1)\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1NjHQfWad2yW3C4USvQ4PCeB9_1-jlFBe)\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(15 points) Consider the confusion matrix below for a machine learning algorithm that recognizes daily activities from motion sensor data. Where needed, treat bed-toilet transition as the positive class.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1kHQz2qYqPZm3L8gWfTA_iasBw84kSPI6)\n",
        "\n",
        "Evaluate the classifier's performance using the following metrics:\n",
        "\n",
        "- accuracy\n",
        "- precision (for bed-toilet transition)\n",
        "- recall (for bed-toilet transition)\n",
        "- macro f-measure\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(12 points) The table below shows how learning algorithms A and B performed for 10 different holdout sets of data. Using these results, determine if the the differences in performance are statistically significant and whether they are extremely statistically significant. Show all of your work.\n",
        "\n",
        "A | B\n",
        "--- | ---\n",
        "5 | 20\n",
        "3 | 13\n",
        "5 | 13\n",
        "12 | 20\n",
        "15 | 29\n",
        "16 | 32\n",
        "17 | 23\n",
        "19 | 20\n",
        "20 | 25\n",
        "24 | 15\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(50 points, and look below the code for Problem 5) The goal of this program is to give you exposure to an existing ranking algorithm and for you to think about alternative ranking evaluation measures. Below you will find an implementation of two ranking algorithms: Naive Rank Train with Naive Rank Test, and RankTrain with RankTest. These are applied to ranking movies based on movie ratings from the MovieLens education ml-latest-small (size: 1MB) dataset found at https://grouplens.org/datasets/movielens/. A cleaned-up version of this data is in the files movies.csv (https://drive.google.com/open?id=1EyNQcP-ES_qCG-ZDzMQnFMOSRH7n0YSw) and ratings.csv (https://drive.google.com/open?id=1b8Emj1UvpWapsC0jkY2gy-FgBKRI5PX2). To augment this dataset, we have collected the plot summary (plots-imdb.csv, https://drive.google.com/open?id=1kviRtGgkybzZvo6xZyuUE9Ic0qKwJx2D) and rating (ratings-imdb.csv, https://drive.google.com/open?id=1j0vXFP5bGTvtq6Fm6nmRViyXAoKXhqeF) available from IMDb for the movies in the MovieLens dataset.\n",
        "\n",
        "In this code, ranking is evaluated using the Kemeny distance measure focused only on the top 50 results, where the IMDb ratings are considered ground truth. Using the Kemeny distance, the average difference is computed between the ranks of each movie. Each pair of movies is represented as a feature vector containing their release years, and bag of words (count vectorizer) for each of their genres, titles and plots.\n",
        "\n",
        "Your task is to add a new evaluation measure in addition to the Kemeny distance measure. The new measure is mean reciprocal rank. This is the multiplicative inverse of the rank of the first correct answer (the first movie that is in the correct ranking location).\n",
        "\n",
        "Report the result of the ranking algorithm for the movie database using both evaluation measures. When would one measure be preferred over the other?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGXUuOEhnzB6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "7de0dd06-4d72-43f5-e92c-ad7cd041bdbd"
      },
      "source": [
        "import sys\n",
        "import csv\n",
        "import random\n",
        "import numpy\n",
        "import scipy\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Global parameters\n",
        "exclude_stop_words = True # If True, then exclude stop words from counts/frequencies\n",
        "use_tfidf = True          # If True, then use word frequencies; else use word counts\n",
        "ranking_limit = 50        # Limit to number of movies in final rankings\n",
        "\n",
        "# Dictionaries to hold data read in from files\n",
        "movie_title = {} # dict of movie title by movieId\n",
        "movie_year = {} # dict of movie year by movieId\n",
        "movie_genres = {} # dict of genres, list of genre keywords, by movieId\n",
        "movie_plot = {} # dict of movie plot by movieId\n",
        "movie_imdb_rating = {} # dict movie IMDb rating by movieId\n",
        "user_ratings = {} # dict of ratings, list of (movieId,rating,timestamp), by userId\n",
        "\n",
        "# Word vectors\n",
        "genres_vect = []\n",
        "titles_count_vect = None\n",
        "titles_tfidf_transformer = None\n",
        "plots_count_vect = None\n",
        "plots_tfidf_transformer = None\n",
        "\n",
        "# Global variables to hold training data\n",
        "X = None # will be a sparse matrix\n",
        "y = []   # list of target classes\n",
        "w = []   # list of sample weights\n",
        "\n",
        "\n",
        "# ----- Data Processing -----\n",
        "\n",
        "def read_data():\n",
        "    global movie_title, movie_year, movie_genres, movie_plot, movie_imdb_rating, user_ratings\n",
        "    # read movie titles, years, and genres\n",
        "    with open('/content/gdrive/My Drive/ML/HW/movies.csv') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        line_num = 0\n",
        "        for row in csv_reader:\n",
        "            if line_num > 0: # skip header\n",
        "                movieId = int(row[0])\n",
        "                title_year = row[1]\n",
        "                genres = row[2]\n",
        "                movie_title[movieId] = title_year[:-7]\n",
        "                movie_year[movieId] = int(title_year[-5:-1])\n",
        "                if genres == \"(no genres listed)\":\n",
        "                    movie_genres[movieId] = []\n",
        "                else:\n",
        "                    movie_genres[movieId] = genres.split('|')\n",
        "            line_num += 1\n",
        "    # read movie plots\n",
        "    with open('/content/gdrive/My Drive/ML/HW/plots-imdb.csv') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        line_num = 0\n",
        "        for row in csv_reader:\n",
        "            if line_num > 0: # skip header\n",
        "                movieId = int(row[0])\n",
        "                plot = row[1]\n",
        "                movie_plot[movieId] = plot\n",
        "            line_num += 1\n",
        "    # read movie IMDb ratings\n",
        "    with open('/content/gdrive/My Drive/ML/HW/ratings-imdb.csv') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        line_num = 0\n",
        "        for row in csv_reader:\n",
        "            if line_num > 0: # skip header\n",
        "                movieId = int(row[0])\n",
        "                rating = float(row[1])\n",
        "                movie_imdb_rating[movieId] = rating\n",
        "            line_num += 1\n",
        "    # read user ratings of movies\n",
        "    with open('/content/gdrive/My Drive/ML/HW/ratings.csv') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        line_num = 0\n",
        "        for row in csv_reader:\n",
        "            if line_num > 0: # skip header\n",
        "                userId = int(row[0])\n",
        "                movieId = int(row[1])\n",
        "                rating = float(row[2])\n",
        "                timestamp = int(row[3])\n",
        "                user_rating = (movieId, rating, timestamp)\n",
        "                if userId in user_ratings:\n",
        "                    user_ratings[userId].append(user_rating)\n",
        "                else:\n",
        "                    user_ratings[userId] = [user_rating]\n",
        "            line_num += 1\n",
        "\n",
        "def generate_feature_vector(example):\n",
        "    # Returns feature vector for example, where example is of the form\n",
        "    # (userId,movieId1,movieId2,rating_diff). The feature vector consists\n",
        "    # of the year, genre bag of words, title bag of words, and plot bag of\n",
        "    # words for both movies. Assumes proper word vectorization processing\n",
        "    # has already been done (extract_text_features() already called).\n",
        "    movie1_fv = movie_feature_vector(example[1])\n",
        "    movie2_fv = movie_feature_vector(example[2])\n",
        "    return movie1_fv + movie2_fv\n",
        "    \n",
        "def movie_feature_vector(movieId):\n",
        "    global movie_year, movie_title, movie_plot, use_tfidf\n",
        "    global titles_count_vect, titles_tfidf_transformer, plots_count_vect, plots_tfidf_transformer\n",
        "    movie_year_fv = [movie_year[movieId]]\n",
        "    genre_fv = genre_feature_vector(movieId)\n",
        "    title_fv = titles_count_vect.transform([movie_title[movieId]])\n",
        "    if use_tfidf:\n",
        "        title_fv = titles_tfidf_transformer.transform(title_fv)\n",
        "    plot_fv = plots_count_vect.transform([movie_plot[movieId]])\n",
        "    if use_tfidf:\n",
        "        plot_fv = plots_tfidf_transformer.transform(plot_fv)\n",
        "    return movie_year_fv + genre_fv + list(title_fv.toarray()[0]) + list(plot_fv.toarray()[0])\n",
        "\n",
        "def genre_feature_vector(movieId):\n",
        "    # Return Boolean vector indicating which genre keywords associated with given movie.\n",
        "    global movie_genres\n",
        "    genre_fv= []\n",
        "    movie_genre_words = movie_genres[movieId]\n",
        "    for genre_word in genres_vect:\n",
        "        if genre_word in movie_genre_words:\n",
        "            genre_fv.append(1.0)\n",
        "        else:\n",
        "            genre_fv.append(0.0)\n",
        "    return genre_fv\n",
        "\n",
        "def extract_text_features(dataset):\n",
        "    # Compute word vectors for movie genres, titles, and plots, for movies in dataset.\n",
        "    # Genre keywords are fixed, but the titles and plots are free-form text,\n",
        "    # so we use NLTK to tokenize words, and then SciKit Learn's text-based\n",
        "    # feature extraction tools to generate the word vectors.\n",
        "    # Assume read_data() has already been called.\n",
        "    global movie_genres, movie_title, movie_plot, genres_vect, titles_count_vect, plots_count_vect\n",
        "    global titles_tfidf_transformer, plots_tfidf_transformer\n",
        "    global exclude_stop_words, use_tfidf\n",
        "    # Get movieIds mentioned in dataset\n",
        "    movieIds = []\n",
        "    for example in dataset:\n",
        "        movieId1 = example[1]\n",
        "        movieId2 = example[2]\n",
        "        if movieId1 not in movieIds:\n",
        "            movieIds.append(movieId1)\n",
        "        if movieId2 not in movieIds:\n",
        "            movieIds.append(movieId2)\n",
        "    # Movie genres\n",
        "    word_set = set()\n",
        "    for movieId in movieIds:\n",
        "        genre_words = movie_genres[movieId] # genres already converted to list of words\n",
        "        word_set = word_set.union(set(genre_words))\n",
        "    genres_vect = list(word_set)\n",
        "    # Movie titles\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    titles_count_vect = CountVectorizer()\n",
        "    titles_count_vect.set_params(tokenizer=tokenizer.tokenize)\n",
        "    if exclude_stop_words:\n",
        "        titles_count_vect.set_params(stop_words='english')\n",
        "    #titles_count_vect.set_params(ngram_range=(1,2)) # include 1-grams and 2-grams\n",
        "    titles_count_vect.set_params(max_df=0.5) # ignore terms that appear in >50% of the titles\n",
        "    titles_count_vect.set_params(min_df=2) # ignore terms that appear in only 1 title\n",
        "    titles = list(map(lambda k: movie_title[k], movieIds))\n",
        "    titles_count_vect.fit(titles)\n",
        "    if use_tfidf:\n",
        "        title_counts = titles_count_vect.transform(titles)\n",
        "        titles_tfidf_transformer = TfidfTransformer()\n",
        "        titles_tfidf_transformer.fit(title_counts)\n",
        "    # Movie plots\n",
        "    plots_count_vect = CountVectorizer()\n",
        "    plots_count_vect.set_params(tokenizer=tokenizer.tokenize)\n",
        "    if exclude_stop_words:\n",
        "        plots_count_vect.set_params(stop_words='english')\n",
        "    #plots_count_vect.set_params(ngram_range=(1,2)) # include 1-grams and 2-grams\n",
        "    plots_count_vect.set_params(max_df=0.5) # ignore terms that appear in >50% of the plots\n",
        "    plots_count_vect.set_params(min_df=2) # ignore terms that appear in only 1 plot\n",
        "    plots = list(map(lambda k: movie_plot[k], movieIds))\n",
        "    plots_count_vect.fit(plots)\n",
        "    if use_tfidf:\n",
        "        plot_counts = plots_count_vect.transform(plots)\n",
        "        plots_tfidf_transformer = TfidfTransformer()\n",
        "        plots_tfidf_transformer.fit(plot_counts)\n",
        "\n",
        "def build_dataset(movie_id_limit=0):\n",
        "    global user_ratings # dict of user ratings, list of (movieId,rating,timestamp), by userId\n",
        "    # Returns dataset consisting of (userId,movieId1,movieId2,rating_diff), where rating_diff\n",
        "    # is (user_rating(movie1) - user_rating(movie2)), assuming the difference is not zero (no ties).\n",
        "    # If movie_id_limit > 0, then examples restricted to movieId's <= movie_id_limit.\n",
        "    dataset = []\n",
        "    posegs = 0\n",
        "    negegs = 0\n",
        "    for userId in user_ratings:\n",
        "        ratings = remove_duplicate_ratings(user_ratings[userId])\n",
        "        for rating1 in ratings:\n",
        "            movieId1 = rating1[0]\n",
        "            r1 = rating1[1]\n",
        "            if (movie_id_limit == 0) or (movieId1 <= movie_id_limit):\n",
        "                for rating2 in ratings:\n",
        "                    movieId2 = rating2[0]\n",
        "                    r2 = rating2[1]\n",
        "                    if (movie_id_limit == 0) or (movieId2 <= movie_id_limit):\n",
        "                        if (movieId1 != movieId2) and (r1 != r2):\n",
        "                            dataset.append((userId,movieId1,movieId2,r1-r2))\n",
        "    print(\"  \" + str(len(dataset)) + \" egs, movie id limit = \" + str(movie_id_limit))\n",
        "    return dataset\n",
        "\n",
        "def remove_duplicate_ratings(ratings):\n",
        "    # If ratings contains multiple ratings for the same movie, then remove all but the most recent rating.\n",
        "    # Each rating is a tuple (movieId,rating,timestamp).\n",
        "    unique_ratings = []\n",
        "    for rating1 in ratings:\n",
        "        movieId1 = rating1[0]\n",
        "        timestamp1 = rating1[2]\n",
        "        best_rating = True\n",
        "        for rating2 in ratings:\n",
        "            movieId2 = rating2[0]\n",
        "            timestamp2 = rating2[2]\n",
        "            if (movieId1 == movieId2) and (timestamp1 < timestamp2):\n",
        "                best_rating = False\n",
        "                break\n",
        "        if best_rating:\n",
        "            unique_ratings.append(rating1)\n",
        "    return unique_ratings\n",
        "\n",
        "def build_training_set(dataset):\n",
        "    # Construct sparse matrix X of feature vectors for each example in dataset.\n",
        "    # Construct target classes y and sample weights w for each example in dataset.\n",
        "    # Dataset entries are (userId,movieId1,movieId2,rating_diff).\n",
        "    # Example features are based on movie1+movie2. Since most feature\n",
        "    # values will be 0.0, and we have a lot of features, need to use\n",
        "    # a sparse matrix. The targets are y=-1 if rating_diff < 0; otherwise y=1.\n",
        "    # Sample weights are based on abs(rating_diff).\n",
        "    global X, y, w\n",
        "    X_data = []\n",
        "    X_row = []\n",
        "    X_col = []\n",
        "    row = 0\n",
        "    num_egs = len(dataset)\n",
        "    for example in dataset:\n",
        "        #if (row % 100) == 0:\n",
        "        #    print(\"  processing example \" + str(row) + \" of \" + str(num_egs))\n",
        "        fvec = generate_feature_vector(example)\n",
        "        col = 0\n",
        "        for fval in fvec:\n",
        "            if fval != 0.0:\n",
        "                X_data.append(fval)\n",
        "                X_row.append(row)\n",
        "                X_col.append(col)\n",
        "            col += 1\n",
        "        row += 1\n",
        "        rating_diff = example[3]\n",
        "        if rating_diff < 0:\n",
        "            y.append(-1) # movie1 rated lower than movie2 by user\n",
        "        else:\n",
        "            y.append(1) # movie1 rated higher than movie2 by user\n",
        "        w.append(abs(rating_diff)) # cost of misclassification proportional to difference in ratings\n",
        "    X_data_arr = numpy.array(X_data)\n",
        "    X_row_arr = numpy.array(X_row)\n",
        "    X_col_arr = numpy.array(X_col)\n",
        "    X = scipy.sparse.csr_matrix((X_data_arr, (X_row_arr, X_col_arr)), shape=(num_egs, len(fvec)))\n",
        "\n",
        "\n",
        "# ----- Ranking Processing -----\n",
        "\n",
        "def get_imdb_ranking(ranking):\n",
        "    # Returns IMDb's ranking of movies in given ranking. Returned ranking is a list\n",
        "    # of (movieId,imdb_rating) pairs, sorted in decreasing order by imdb_rating.\n",
        "    global movie_imdb_rating\n",
        "    imdb_ratings = list(map(lambda pair: (pair[0], movie_imdb_rating[pair[0]]), ranking))\n",
        "    imdb_ranking = sorted(imdb_ratings, key=lambda pair: pair[1], reverse=True)\n",
        "    return imdb_ranking\n",
        "\n",
        "def print_ranking(ranking):\n",
        "    # Print ranking, which is a list of (movieId,score) pairs, sorted in decreasing order by score.\n",
        "    rank = 1\n",
        "    for movie_score_pair in ranking:\n",
        "        movieId = movie_score_pair[0]\n",
        "        score = movie_score_pair[1]\n",
        "        title = movie_title[movieId]\n",
        "        print(str(rank) + \". (\" + str(score) + \") \" + title + \" (id=\" + str(movieId) + \")\")\n",
        "        rank += 1\n",
        "\n",
        "def compare_rankings(ranking1, ranking2):\n",
        "    # Return evaluation of comparing given rankings (ordered lists of (movieId,score) pairs).\n",
        "    # Specifically, compute the average distance between each movie's ranking.\n",
        "    # This is the Kemeny distance measure. The function assumes the same\n",
        "    # movieIds are in both rankings.\n",
        "\n",
        "\n",
        "    global movie_imdb_rating\n",
        "    movieIds1 = list(map(lambda pair: pair[0], ranking1))\n",
        "    movieIds2 = list(map(lambda pair: pair[0], ranking2))\n",
        "    distance = 0\n",
        "    rank1 = 0\n",
        "    for movieId in movieIds1:\n",
        "        rank2 = movieIds2.index(movieId)\n",
        "        distance += abs(rank1 - rank2)\n",
        "        rank1 += 1\n",
        "    return float(distance) / float(len(ranking1))\n",
        "\n",
        "\n",
        "# ----- Naive Ranking -----\n",
        "\n",
        "def naive_rank_train (classifier):\n",
        "    # Assumes build_training_set already called to build X and y.\n",
        "    return classifier.fit(X,y)\n",
        "\n",
        "def naive_rank_test(dataset, classifier):\n",
        "    # Return list of (movieId, score) pairs sorted in decreasing order by score.\n",
        "    # The classifier is used to predict preference between each pair of movies.\n",
        "    # initialize movie scores dictionary\n",
        "    scores_dict = {}\n",
        "    for example in dataset:\n",
        "        movieId1 = example[1]\n",
        "        movieId2 = example[2]\n",
        "        if movieId1 not in scores_dict:\n",
        "            scores_dict[movieId1] = 0\n",
        "        if movieId2 not in scores_dict:\n",
        "            scores_dict[movieId2] = 0\n",
        "    # update movie scores based on all possible pairs of movies\n",
        "    for movieId1 in scores_dict:\n",
        "        for movieId2 in scores_dict:\n",
        "            if movieId1 != movieId2:\n",
        "                dummy_example = (0,movieId1,movieId2,0)\n",
        "                fv = generate_feature_vector(dummy_example)\n",
        "                y = classifier.predict([fv])[0]\n",
        "                scores_dict[movieId1] += y\n",
        "                scores_dict[movieId2] -= y\n",
        "    # sort movies by score\n",
        "    sorted_scores = sorted(scores_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
        "    return sorted_scores\n",
        "\n",
        "# ----- Ranking v2 -----\n",
        "\n",
        "def rank_train_2 (classifier):\n",
        "    # Assumes build_training_set already called to build X, y, and w.\n",
        "    # The use of w is the main difference between rank_train_2 and naive_rank_train.\n",
        "    global X, y, w\n",
        "    return classifier.fit(X, y, sample_weight=w)\n",
        "\n",
        "def rank_test_2(dataset, classifier):\n",
        "    # Return list of (movieId, score) pairs sorted in decreasing order by score.\n",
        "    # The classifier is used to predict preference between each pair of movies.\n",
        "    # initialize movie scores dictionary\n",
        "    movieIds = []\n",
        "    for example in dataset:\n",
        "        movieId1 = example[1]\n",
        "        movieId2 = example[2]\n",
        "        if movieId1 not in movieIds:\n",
        "            movieIds.append(movieId1)\n",
        "        if movieId2 not in movieIds:\n",
        "            movieIds.append(movieId2)\n",
        "    ranked_movieIds = rank_test_2_quicksort(movieIds, classifier)\n",
        "    # Rank Test v2 doesn't produce movie ranking scores, so just set each movie's score to 0\n",
        "    return list(map(lambda id: (id,0), ranked_movieIds))\n",
        "\n",
        "def rank_test_2_quicksort(movieIds, classifier):\n",
        "    if len(movieIds) < 2:\n",
        "        return movieIds\n",
        "    else:\n",
        "        pivot = random.choice(movieIds)\n",
        "        left = []\n",
        "        right = []\n",
        "        for movieId in movieIds:\n",
        "            if movieId != pivot:\n",
        "                dummy_example = (0,movieId,pivot,0)\n",
        "                fv = generate_feature_vector(dummy_example)\n",
        "                class_1_index = numpy.where(classifier.classes_ == 1)[0]\n",
        "                y_prob = classifier.predict_proba([fv])[0][class_1_index] # probability movie preferred to pivot\n",
        "                if numpy.random.uniform(0,1) < y_prob:\n",
        "                    left.append(movieId)\n",
        "                else:\n",
        "                    right.append(movieId)\n",
        "        left = rank_test_2_quicksort(left, classifier)\n",
        "        right = rank_test_2_quicksort(right, classifier)\n",
        "        return left + [pivot] + right\n",
        "\n",
        "\n",
        "# ----- Main -----\n",
        "\n",
        "def main():\n",
        "    global movie_title, ranking_limit\n",
        "    #print(\"Reading data...\", flush=True)\n",
        "    read_data()\n",
        "    #print(\"Building dataset...\", flush=True)\n",
        "    dataset = build_dataset(200)   # parameter is number of movies\n",
        "    #print(\"Extracting text features...\", flush=True)\n",
        "    extract_text_features(dataset)\n",
        "    #print(\"Building training set...\", flush=True)\n",
        "    build_training_set(dataset)\n",
        "    \n",
        "    # Sample for testing\n",
        "    example = dataset[0]\n",
        "    #print(\"  Example: \" + str(example), flush=True)\n",
        "    fv = generate_feature_vector(example)\n",
        "    #print(\"  Feature vector (#features = \" + str(len(fv)) + \"):\", flush=True)\n",
        "    #print(\"  \" + str(fv))\n",
        "    \n",
        "    # Naive Ranking\n",
        "    print(\"\\nNaive Ranking: training classifier...\", flush=True)\n",
        "    classifier = MultinomialNB()\n",
        "    classifier = naive_rank_train(classifier)\n",
        "    naive_ranking = naive_rank_test(dataset, classifier)[:ranking_limit]\n",
        "    imdb_ranking = get_imdb_ranking(naive_ranking)\n",
        "    dist = compare_rankings(naive_ranking,imdb_ranking)\n",
        "    print(\"\\nNaive Ranking (distance to IMDb ranking = \" + str(dist) + \"):\", flush=True)\n",
        "    print_ranking(naive_ranking)\n",
        "    print(\"IMDb Ranking:\", flush=True)\n",
        "    print_ranking(imdb_ranking)\n",
        "    \n",
        "    # Ranking v2\n",
        "    print(\"\\nRanking v2: training classifier...\", flush=True)\n",
        "    classifier = MultinomialNB()\n",
        "    classifier = rank_train_2(classifier)\n",
        "    ranking2 = rank_test_2(dataset, classifier)[:ranking_limit]\n",
        "    imdb_ranking2 = get_imdb_ranking(ranking2)\n",
        "    dist = compare_rankings(ranking2,imdb_ranking2)\n",
        "    print(\"\\nRanking v2 (distance to IMDb ranking = \" + str(dist) + \"):\", flush=True)\n",
        "    print_ranking(ranking2)\n",
        "    print(\"IMDb Ranking:\", flush=True)\n",
        "    print_ranking(imdb_ranking2)\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e8bd1a385f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mprint_ranking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_ranking2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-e8bd1a385f1a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mmovie_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranking_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;31m#print(\"Reading data...\", flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0;31m#print(\"Building dataset...\", flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# parameter is number of movies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e8bd1a385f1a>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mmovie_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_year\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_imdb_rating\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_ratings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# read movie titles, years, and genres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/ML/HW/movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/ML/HW/movies.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U9aOmOSWtEm"
      },
      "source": [
        "#5.\n",
        "\n",
        "(50 points) The goal of this problem is to give you coding experience with k-means and to see how clustering can be used for image segmentation. The code provided below reads in pixel information for this image:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1I2owDFxN1yJW63FnQJUsa-kh-aS8nQ2W)\n",
        "\n",
        "The code calls k-means to cluster individual pixels into 3 groups. The pixels will then be mapped onto one of three colors, corresponding to the cluster number, and the new image will be displayed. Depending on how the clustering performs, you might see trees rendered as one color, water as another, and sky as yet another color.\n",
        "\n",
        "Most of the code is provided for you, including a call to kmeans, a calculation of euclidean distance between two pixels, and an initialization of the clusters. You need to provide the code following the comment ``TO DO''. Specifically, for this problem you need to update the cluster centers and redetermine pixel labels until the max_iterations threshold is reached.\n",
        "\n",
        "Reflect on how your clustering algorithm segmented the image. What are ways you feel performance could be improved?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_D_qZjfX26M"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "def euclidean_distance(x, y):\n",
        "  distance = (x[0] - y[0])**2 + (x[1] - y[1])**2 + (x[2] - y[2])**2\n",
        "  return math.sqrt(distance)\n",
        "\n",
        "\n",
        "def kmeans(data, k, max_iterations):\n",
        "  random.seed()\n",
        "  n = len(data)\n",
        "  centers = np.zeros((k, 3))\n",
        "  for i in range(k):   # pick random points for initial centers\n",
        "    r = random.randrange(0, n-1, 1)\n",
        "    centers[i] = data[r]\n",
        "  labels = np.zeros((len(data), 1), dtype=int) # initial cluster memberships\n",
        "  # update the clusters until max_iterations is reached\n",
        "  # TO DO\n",
        "  return labels, centers\n",
        "\n",
        "\n",
        "def main():\n",
        "  image = cv2.imread('/content/gdrive/My Drive/ML/HW/image.png')\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # map onto our color space\n",
        "\n",
        "  # reshape the image to a 2D array of pixels and 3 color values (RGB)\n",
        "  pixel_values = image.reshape((-1, 3))   # Here -1 means unknown dimension\n",
        "  # convert to float\n",
        "  pixel_values = np.float32(pixel_values)\n",
        "\n",
        "  # define stopping criteria\n",
        "  criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
        "\n",
        "  k = 3\n",
        "  max_iterations = 0\n",
        "  labels, (centers) = kmeans(pixel_values, k, max_iterations)\n",
        "  # you can uncomment this next line to see how it could look after kmeans\n",
        "  #_, labels, (centers) = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "  # convert back to 8 bit values\n",
        "  centers = np.uint8(centers)\n",
        "\n",
        "  # flatten the labels array\n",
        "  labels = labels.flatten()\n",
        "  # convert all pixels to the color of the centroids\n",
        "  segmented_image = centers[labels.flatten()]\n",
        "\n",
        "  # reshape back to the original image dimension\n",
        "  segmented_image = segmented_image.reshape(image.shape)\n",
        "  # show the image\n",
        "  plt.imshow(segmented_image)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fEEIIAEvVGz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}